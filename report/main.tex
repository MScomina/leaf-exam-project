\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{paralist}
\usepackage{color}
\usepackage[detect-weight=true, binary-units=true]{siunitx}
\usepackage{pgfplots}
\usepackage{authblk}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{hyperref}

\title{Machine Learning and Data Mining project:\\Leaf identification}
\author[1]{Michele Scomina}
\affil[1]{
    problem statement,
    solution design,
    solution development,
    data gathering,
    writing
}
\date{Course of AA $2023$-$2024$ - DIA - Ingegneria Elettronica e Informatica (IN20)}



\begin{document}

\maketitle



\section{Problem statement}
\subsection{Goal of the project}
The goal of this project is to identify the type of leaf based on a group of features, which are derived from both the shape and texture of the leaf.
The given solution must be able to classify the leaves in one of 30 categories of leaves.

\subsection{Formal definitions}
The problem can be formally defined as follows:
\begin{itemize}
    \item Let $X$ be the set of all possible leaves. Every element $x \in X$ is a leaf defined by a 14-dimensional attribute vector, representing various shape and texture features of the leaf.
    \item Let $Y$ be the set of all possible categories of leaves. Every element $y \in Y = \{0, 1, 2, ..., 29\}$ represents a unique category of leaves.
    \item Let $f: X \to Y$ be the function that assigns each leaf to its category based on its attributes.
    \item Let $D = \{(x^{(1)}, y^{(1)}), \ldots, (x^{(n)}, y^{(n)})\}$ be the dataset, where $x^{(i)} \in X$ and $y^{(i)} \in Y$.
    \item The goal is to learn a model $M$ from the dataset $D$ such that the argmax of $f'_{\text{predict}}: X \times M \to P_{Y}$\footnote{...where $P_{Y}$ is the set of all possible probability distributions over $Y$.} approximates $f$ as closely as possible, effectively classifying the leaves into their correct categories.
\end{itemize}


\section{Assessment and performance indexes}
Since classes are relatively balanced, but are expected to be composed of only a few samples, there's a risk of ill-defined metrics, 
as the model might not be able to predict some classes and might not have a properly defined precision, recall or F1-score for those classes.
Therefore, the weighted accuracy and the AUC (OvR) will be used as performance indexes. 
The confusion matrices and the multiclass ROC (OvR)$^{\cite{multiclass_roc}}$ will also be used to better assess the models' performances.


\section{Proposed solution}
\subsection{Data pre-processing}
In order to have any sort of meaningful evaluation of the models, the dataset $D$ has to be split into a training set $D_{\text{train}}$ and a test set $D_{\text{test}}$, 
in order to test the models for generalization on unseen data:

\begin{equation*}
    D_{\text{train}} = subbag(D, r) \quad \text{and} \quad D_{\text{test}} = D \setminus D_{\text{train}}
\end{equation*}

The split will be done with a $80$-$20\%$ ratio (r = 0.8) using a stratified shuffle, in order to avoid misrepresentation of classes:

\begin{equation*}
    \frac
        {|\{(x^{(i)}, y^{(i)}) \in D_{\text{train}} : y^{(i)} = c\}|}
        {|D_{\text{train}}|}
    \approx 
    \frac
        {|\{(x^{(i)}, y^{(i)}) \in D_{\text{test}} : y^{(i)} = c\}|}
        {|D_{\text{test}}|},
    \quad \forall c \in Y
\end{equation*}

\subsection{Types of models}
For this project, three different kinds of models will be trained, in order to determine which one performs best for the problem at hand:
\begin{itemize}
    \item \textbf{Random Forest}, $M = \{T_1, T_2,... T_k\}$ with each $T_i \in T_{(S_i \times \mathbb{R}) \cup P_{Y}}$, where $S_i \subset \{1,...,14\}$ is a random subset of 4 features.\footnote{The number of features was chosen based on the square root of the total number of dimensions ($\lceil\sqrt{14}\rceil = 4$).}
    \item \textbf{Soft Support Vector Classifier (OvR)}, $M = \{w, b\}$ such that $w \in \mathbb{R}^{14}$ and $b \in \mathbb{R}$.
    \item \textbf{Naive Bayes}, $M = \{P(Y), P(X|Y)\}$, with $P(Y)$ being the prior probabilities of the classes in $Y$, and $P(X|Y)$ being the likelihood of the features in $X$ given the classes in $Y$.
\end{itemize}
One thing to note is that, normally, multiclass SVCs do not output probabilities.
Therefore, the SVC will be used in conjunction with Platt scaling$^{\cite{platt1999probabilistic}\cite{wu2003probability}}$, in order to follow the formal definition of the problem of outputting probability distributions and calculate the multiclass ROC and AUC.


\subsection{Model training}
The hyperparameters will first be searched for each model using a grid search with a $5$-fold cross-validation on the training set\footnote{...if applicable.}:
\begin{equation*}
    P^{*} = 
    (p_{1}^{*}, p_{2}^{*}, \dots, p_{h}^{*}) = 
    \underset{p_{1}, p_{2}, \dots, p_{h}}{\text{argmax}}
        \left(\frac{1}{5}\sum_{i=1}^{5}\text{AUC}_{\text{OvR}}
            (M(p_{1}, p_{2}, \dots, p_{h}), 
            D_{\text{train}}^{(i)})
        \right)
\end{equation*}
The models will then be trained on the entire training set with the best hyperparameters found during the grid search:
\begin{equation*}
    M^{*} = f_{\text{learn}}{(D_{\text{train}}, P^{*})}
\end{equation*}



\section{Experimental evaluation}



\subsection{Data}
The dataset used for this project is the "leaf" dataset\footnote{https://archive.ics.uci.edu/dataset/288/leaf}, which comprises of 340 samples of simple leaves, evenly distributed among 30 classes.



\subsection{Procedure}
Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Donec euismod nibh vitae elit dignissim, a maximus justo mattis.
Cras elementum dolor at volutpat lacinia.
Duis a magna tempus, lobortis arcu eget, blandit augue.
Nulla facilisi.
Sed nibh lorem, tempus eget pellentesque vel, sagittis id quam.
Mauris interdum faucibus vulputate.
Nunc a ex tincidunt, scelerisque turpis a, dictum metus.
Nunc nec dui dui.
Ut mollis massa ligula, at pretium nibh tristique at.
Mauris suscipit fermentum congue.
Vivamus in auctor nibh.
Vestibulum magna nisl, blandit luctus eleifend vitae, pharetra et ipsum.
Duis faucibus vulputate mauris, eu viverra elit mattis vel.

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Donec euismod nibh vitae elit dignissim, a maximus justo mattis.
Cras elementum dolor at volutpat lacinia.
Duis a magna tempus, lobortis arcu eget, blandit augue.
Nulla facilisi.
Sed nibh lorem, tempus eget pellentesque vel, sagittis id quam.
Mauris interdum faucibus vulputate.
Nunc a ex tincidunt, scelerisque turpis a, dictum metus.
Nunc nec dui dui.
Ut mollis massa ligula, at pretium nibh tristique at.
Mauris suscipit fermentum congue.
Vivamus in auctor nibh.
Vestibulum magna nisl, blandit luctus eleifend vitae, pharetra et ipsum.
Duis faucibus vulputate mauris, eu viverra elit mattis vel.



\subsection{Results and discussion}
Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Donec euismod nibh vitae elit dignissim, a maximus justo mattis.
Cras elementum dolor at volutpat lacinia.
Duis a magna tempus, lobortis arcu eget, blandit augue.
Nulla facilisi.
Sed nibh lorem, tempus eget pellentesque vel, sagittis id quam.
Mauris interdum faucibus vulputate.
Nunc a ex tincidunt, scelerisque turpis a, dictum metus.
Nunc nec dui dui.
Ut mollis massa ligula, at pretium nibh tristique at.
Mauris suscipit fermentum congue.
Vivamus in auctor nibh.
Vestibulum magna nisl, blandit luctus eleifend vitae, pharetra et ipsum.
Duis faucibus vulputate mauris, eu viverra elit mattis vel.

\begin{table}[]
    \centering
    \begin{tabular}{
        c S[table-format = 3.0] |
        S[table-format = 1.2] S[table-format = 1.2] S[table-format = 4.0] S[table-format = 1.2] S[table-format = 4]
    }
        \toprule
        {$n_\text{this}$} & {$n_\text{that}$} & {LF} & {TF} & {\#I} & {\#S [$\times 10^6$]} & {$t_l$ [\si{\second}]}\\
        \midrule
        \multirow{3}{*}{50}
        & 10 & 0.37 & 0.45 & 552 & 0.59 & 52 \\
        & 25 & 0.43 & 0.44 & 3076 & 0.56 & 245 \\
        & 50 & 0.45 & 0.43 & 637 & 0.64 & 715 \\
        \midrule
        \multirow{3}{*}{100}
        & 10 & 0.34 & 0.50 & 1138 & 2.76 & 110 \\
        & 25 & 0.40 & 0.48 & 1224 & 0.94 & 326 \\
        & 50 & 0.38 & 0.49 & 443 & 0.44 & 1056 \\
        \bottomrule
    \end{tabular}
    \caption{
        Results (including learning time $t_l$) for different values of $n_\text{this}$ and $n_\text{that}$.
    }
    \label{tab:experiments}
\end{table}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Donec euismod nibh vitae elit dignissim, a maximus justo mattis.
Cras elementum dolor at volutpat lacinia.
Duis a magna tempus, lobortis arcu eget, blandit augue.
Nulla facilisi.
Sed nibh lorem, tempus eget pellentesque vel, sagittis id quam.
Mauris interdum faucibus vulputate.
Nunc a ex tincidunt, scelerisque turpis a, dictum metus.
Nunc nec dui dui.
Ut mollis massa ligula, at pretium nibh tristique at.
Mauris suscipit fermentum congue.
Vivamus in auctor nibh.
Vestibulum magna nisl, blandit luctus eleifend vitae, pharetra et ipsum.
Duis faucibus vulputate mauris, eu viverra elit mattis vel.



\bibliographystyle{plain}
\bibliography{references}

\end{document}
